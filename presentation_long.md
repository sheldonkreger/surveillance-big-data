# Presentation

## Introduction

The year is 2029, and political tensions in the United States have reached a tipping point. Unemployment has recently reached 15%, with truckers replaced by autonomous vehicles, cashiers replaced by RFID scanners, and call centers replaced with deceptively responsive converstaion bots. Next year, Silicon Valley startup CyberLife will drop the cost of its general-purpose, humanoid robot from $75,000 per unit to just $15,000, which could push people in manufacturing, janitorial, landscaping, and farming out of work.

The political responses have been intense, but there is no universal agreement upon where to place the blame, or what to do. Instead, a gridlock between conservatives and liberals in the US has formed. Conservatives believe the virtue of the market, and reject the idea of restraining emerging technologies or ensuring living conditions for the growing number of homeless. Liberals see believe in the virute of the technology to alleviate humans from the burden of labor, but have not been successful in protecting the unemployed or in helping displaced workers retrain and find new jobs.

Robert, a 27 year old white man living in Chicago, has grown so uneasy that he recently shoved a man halfway across the bus after a brief argument about a seat. Robert, already struggling to make rent, lost the automated bid on a key project and overboiled when the suit-and-collar man who tried to shove his way into the last seat. He had been banned from the bus system for a few weeks, but never faced any criminal charges.

Robert's problems came from every direction at once. After his sister was killed in the previous year in an autonomous vehicle accident, the courts ruled in favor of the manufacturer. And, after spending months sending messages to women through online dating apps, had finally realized he had been 'shadow banned', where his profile was only matched with inactive accounts and his messages ended up in a virutal trash bin. It had been nearly four years since he'd had sex, and the sum of all these anxieties was taking its toll.

The algorithms had taken his sister, his work, and his love life away. Looking around, he knew he wasn't alone in his desparate loneliness.

The only good news for Robert was the recent passing of his grandmother, who left behind a hefty estate from her Los Angeles home, which was sold and the earnings split between Robert and his parents. This was enough for Robert to finally move on, to move *away*, and to finally make a statement that nobody would forget.

One of Robert's hobbies was drone photography, and he was active in a number of online groups. Over the last few years, he had seen many drone customizations, and even tried a few himself. One technique was using micro-drones in a swarm configuration to surround a person with cameras and render a 3D image. Robert's creativity was outpaced only by his growing anger, and over the last year or so, he'd been looking at how weapons could be attached to drones and piloted for destruction. This had spawned a plan for revenge against the industry that had destroyed his life.

With his newly acquired funds, he went online and ordered a small army of high-speed microdrones - each about three inches in length. He also purchased an industrial 3D printer, which would easily render long, razer-sharp nails and brackets to attach to each drone. He ordered a total of 10,000 drones from 30 suppliers, some domestic and some international, and also downloaded the bluebrints for the 3D printer. A custom script he crafted utilized facial attributes to target middle aged, white and Asian males - the demographic of Silicon Valley engineers. After a few weeks of assembly and some test runs, Robert rented three vans in San Franscisco, loaded them with the drones, and at 11:30am on a Tuesday, released an unprecedented force of violence in the city.

Although my writing is certainly not on par-with that of the Netflix series *Black Mirror*, I do think that Robert's story can help us gain perspective of the growing power of individual actors and how surveillance might be used to thwart them. First, we'll explore the philisophical argument that science will inevitably create technologies which create existential risk. Second, we'll explore some of the most immediately emerging threats in this space, and argue that surveillance will be a critical line of defense. Third, we will point out today's current misuse of surveillance - we'll detail how private companies currently exploit their customers through surviellance and big data systems, actively manipulating people to purchase products or participate in other agendas through a process called *instrumentarianism*. Third, we will see how governmental surveillance is legally distinguished from these corporate endeavors, but that corporate data is invaluable for thwarting future threats. Finally, we will look at a real-world example of a state/corporate hybrid surveillance system and its controversial nature.

## Introduction, Short

Any fan of the Netflix original series "Black Mirror" has thought about some frightening, technology-driven scenarios we could face in the near future. If you're not familiar with the series, allow me to share a storyline I conjured up on my own.

Imagine the year 2029, and a young man has been pushed into desparation. More often than not, he loses bids on freelance software projects to overseas competitors. His online dating profile was recently shut down due to an automated algorithm which accused him of misbehaving. Furthermore, his younger sister was killed in a car crash involving automated vehicles. The algorithms have taken his work, his sex life, and his sister. Based on the kinds of political articles he reads, he is suggested an online group of blossoming extremists, who plan an attack on Silicon Valley using an army of small attack drones - built with readily available kits which they modify with 3D printers and custom software.

Although my writing is certainly not on par-with that of the Netflix series *Black Mirror*, I do think that Robert's story can help us gain perspective of the growing power of individual actors and how surveillance might be used to thwart them. First, we'll explore the philisophical argument that science will inevitably create technologies which create existential risk. Second, we'll explore some of the most immediately emerging threats in this space, and argue that surveillance will be a critical line of defense. Third, we will point out today's current misuse of surveillance - we'll detail how private companies currently exploit their customers through surviellance and big data systems, actively manipulating people to purchase products or participate in other agendas through a process called *instrumentarianism*. Third, we will see how governmental surveillance is legally distinguished from these corporate endeavors, but that corporate data is invaluable for thwarting future threats. Finally, we will look at a real-world example of a state/corporate hybrid surveillance system and its controversial nature.

## Vulnerable World Hypothesis

Oxford philosopher Nick Bostrom specializes in the ethics of near-future problems such as human enhancement, artificial intelligence, and existential risk. In a recent paper, he lays the philosophical groundwork to argue that the scientific method will inevitably lead to technologies which pose danger to the survival of humanity.

Imagine an urn filled with balls that are white, black, and various shades of grey. Each time an invention is brought to life, humanity reaches into the urn and draws a ball. The white balls represent technologies which are generally positive for humanity, while the black balls represent inventions which lead to extinction. And, of course, there are many grey balls which exhibit both positive and negative traits. Thankfully, most of the balls so far have been grey or white. Bostrom, however, believes that in the near future, we will be pulling some dark grey or even black balls, and without some kind of social framework to address them, this will result in a major catastrophe.

For example, the rapid developments in the biotech industry have enabled smaller labs to conduct more powerful research. Restrictions on technologies used to manufacture, say, a lethal virus, are currently inadequate in context of the amount of damage such a bioweapon could cause. Each year, these materials become accessible to more people at a lower cost, making it more and more likely that a rogue agent or small sect will implement a bioweapon attack. Bostrom calls these kinds of threats 'easy nukes' and we will explore further examples later.

> Intuitively, the hypothesis is that there is some level of technology at which civilization almost certainly gets destroyed unless quite extraordinary and historically unprecedented degrees of preventive policing and/or global  governance are implemented. More precisely:
>>
> -VWH: If technological development continues then a set of capabilities will at some point be attained that make the devastation of civilization extremely likely, unless civilization sufficiently *exits* the semi-anarchic default condition. (pg 6)

The "semi-anarchic default conditions" are:

> - Limited capacity for preventive policing. States do not have sufficiently reliable means of real-time surveillance and interception to make it virtually impossible for any individual or small group within their territory to carry out illegal actions—particularly actions that are very strongly disfavored by >99% of the population.
> - Limited capacity for global governance. There is no reliable mechanism for solving global coordination problems and protecting global commons—particularly in high-stakes situations where vital national security interests are involved.
> - Diverse motivations. There is a wide and recognizably human distribution of motives represented by a large population of actors (at both the individual and state level)—in particular, there are many actors motivated, to a substantial degree, by perceived self-interest (e.g. money, power, status, comfort and convenience) and there are some actors (“the apocalyptic residual”) who would act in ways that destroy civilization even at high cost to themselves. (pg 7)

It is important to frame the conversation around surveillance, big data, and privacy from this perspective. Later, I will argue that although such threats may justify a broad expansion of state authority and diminished privacy, they do not justify universal application of these tools to general criminal justice or corporate profiteering.

## Emerging Grey Orbs

In their book "The Future of Violence", Gabriella Blum (Harvard Law Professor) and Benjamin Wittes (Senior Fellow in Governance Studies at the Brookings Institution) outline 'technologies of empowerment' which will give individuals or small groups the capability to cause major catastrophes. These fall into four categories:

- Bioweapons, where an individual uses lab equipment to create a virus or other infectious agent. Such equipment could be used at work in a commercial setting, at a university in a research setting, and as time passes, lab equipment could be come more affordable for individuals to own. Advanced manipulations to viruses and other bioweapons will become easier to execute with less equipment and knowledge.
- Cyberspace (networked computers), where individuals can already mount sizeable infrastructure attacks (such as a distributed denial of service or DDOS) with minimal technical knowledge. Cyber attacks threaten software systems which control life-supporting technologies such as electrical grids, and also military equipment such as control systems for nuclear weapons. The recent expansion of cyberspace has created a vast domain of attack vectors, each of which is potentially exploitable by an individual rogue actor.
- Robotics, in which a person could piece together pre-manufactured components to create something unforseen and dangerous. Or, they might reprogram a fully operational unit to perform tasks outside its intended purpose. For example, a small army of drones with guns attached to them. Military drones could also be hacked and and used by nefarious actors. Another example in the book is "Metal Storm", which was created by a grocery store worker. It's a gun that uses electricy rather than gunpowder, which results in an astonishing rate of fire - up to 1 million rounds per minute.
- Nanotechnology, which is in early stage and not yet under the realm of 'mass empowerment.' Surveillance data, ripe for exploitation, will grow exponentially in size and detail. Manufacturing of virtually all goods will be radically changed as the technology develops, not only because of revolutionary new materials, but because constuction of components can be decentralized: Imagine a nanotechnology 'printer' which can create - atom by atom - all of your household goods from basic elements like carbon. This of course creates unprecedented risks for weapons manufacturing. Furthermore, nano-sized robots could be used in a 'swarm' to attack infrastructure or people.

## Real-world Example

We have already seen examples of individuals manufacturing weapons at home using 3D printers. The first person in the UK to be convicted for using a 3D printer to manufacture a gun was Tendai Muswere, 26, and he pleaded guilty at Southwark crown court on Wednesday to the charge of manufacturing a 3D printed gun. A subsequent police search of Muswere’s browsing history revealed he had watched videos demonstrating how to manufacture a firearm capable of firing live ammunition using a 3D printer.

Thankfully, such weapons do not pose unprecedented risks - and they actually look quite pathetic in comparison to what we can buy from dealers here in the US. But the underlying process of downloading instructions and creating weapons at home will become a serious problem in the near future.

## Corporate Enforcement

The question, of course, is how to best respond to this challenge. One may be tempted to place the responsibility on internet corporations to patrol forums and shut down illegal activity as it arises. This is the existing way of doing things, and as we can see, it leaves too much room for imperfect policing, depending on the desires and resources of the companies in question. They have also failed to retain security over sensitive data. And, the data required to end Bostrom's 'semi-anarchic state' will be extremely diverse and sensitive in nature - so much so, in fact, that many companies today try to actively avoid cooperation with law enforcement due to the way it affects their public reputation.

But, there are many other reasons why this will not work. My argument is that relying upon tech companies to enforce policies designed to prevent massive catastrophes is not only inneffective, but it is extremely dangerous. This is because such bohemoths have historically abused the data they collect on individuals in order to exploit them later on. Fundamentally, their goal is not the protection of the public good, but rather to perpetually create profits.

## Surveillance Capitalism

Shoshana Zuboff, one of Havard Business School's first tenured female professors, provides a impressive account of corporate leveraging of big data in her 2019 masterpiece *The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power*. Having spent more than a decade interviewing top data scientists, engineers, and exectuives at major tech companies, Zuboff has documented the emergence of an unprecedented form of power called *instrumentarianism*.

## Instrumentarianism

Referencing several public lectures, presentations, and articles from tech industry leaders, Zuboff clarifies a narrative in which Silicon Valley businesses are racing for a complete system of surveillance and analysis. Through a rapidly expanding network of sensors, companies collect behavioral data and make predictions about future behavior. This data is currently used primarily to target advertisements to the consumers who are most likely to buy a product, based on mathematical analysis of previous behaviors. This process Zuboff calls *the prediction imperative* and is paired with the *extraction imperative*, or the endless drive to amass behavioral data. And, of course, the unrestrained nature of capitalism means these imperatives have no boundaries, as long as they remain profitable.

## Economies of Action

We are familiar with these systems in our every day lives, when we visit specific web pages and later recieve advertisements about those topics. Zuboff's work reveals, however, that the agenda reaches far beyond targeted advertising, but toward behavioral modification in general. Although profitability remains a top priority in surveillance capitalism, there are noteworthy techniques at play in later developments:

- *Tuning* involves queuing or nudging a user to follow a series of actions at a precise time and place.
- *Herding* involves controlling key aspects of a person's immediate context, and therefore forcing them to take a specific action. Disallowing a drunk person from starting their car (based on sensor data) would push the person out of the car toward purchasing a ride.
- *Conditioning* is a long-standing appraoch to modifying behavior, most often associated with Harvard Psychologist BF Skinner. By providing environmental conditions and reinforcements for specific sequences of behavoir, individuals can be modified to perform specific routines. Skinner called this manipulation *behavioral engineering* and the work of BF Skinner has been a major influence among tech industry leaders.

## Recent Publications

One controversial example of tuning was detailed in a 2012 article in *Nature*, where Facebook engineers had manipulatad 61 million individual news feeds in the run-up to the 2010 congressional election. 

> One group was shown a statement at the top of their news feed encouraging the user to vote. It included a link to polling place information, an actionable buttion reading "I Voted," a counter indicating how manyu other Facebook users reported voting, and up to six profile pictures of the user's Facebook friends who had already clicked the "I Voted" button. A second group received the same information but without the pictures of friends. A third control group did not receive any special message.
> 
> . . . Users who recieved the social message wer about 2 percent more likely to click on the "I Voted" button than those who received the information alone and 0.26 more likely to click on polling place information. Facebook experimenters determined that social messageing was an effective means of tuning behavior at scale because it "directly influenced political self-expression, information seeking and real-world voting behavior of millions of people," and they concluded that "showing familiar faces to users can dramatically improve the effectiveness of a mobilization message."

There are more and more examples of these kinds of studies in this explosive domain of sociology. While it may seem far-fetched to think these companies can dictate our behavior, take a moment to think about the influence they already have over us. Have you ever chosen a restaurant based on Yelp reviews? Or maybe you found a doctor with a Google search? Any of your friends meet their partners through dating apps? When your cousin was married, did you find out on Facebook, or did they send you a letter first? Have you ever chosen a different route or completely changed your plans because of traffic alerts sent to your phone? We have been conditioned to these seemingly minor changes over the period of 10 or 15 years. Individually, they seem benign, but cumulitavely it represents an unprecedented shift in power which Zuboff argues is on the same scale of the rise of Totalitarianism in the 1900s (but, thankfully, without the physical violence). Rather than being convinced to join a cause and fight a political battle, we are instead *radically indifferent*, responding to the convenient queues surrounding us and then submitting our behavioral data, in an endless cycle to concentrate power in ways never before seen.

## Convergence

On the one hand, we see the inevitable rise of *technologies of mass empowerment*, which will allow small groups or individual rogue actors to execute catastrophic attacks against large populations. On the other hand, we see the broadening power of corporations through the manipulation of individual behavior. This leaves us one final area to examine, which is the government's surveillance activities.

Under the Foreign Intelligence Surveillance Act (FISA), Section 702, government agencies are allowed to surveil any non-US citizens outside the country. Spying on adversaries is standard procedure in countries around the world, and the US has maintained this capability for many reasons and it remains largely uncontroversial. In contrast, surveillance targetting US citizens is extremely controversial because of fourth ammendment protections. This surprising double-standard has created logistical complications which are unthinkable in other countries in the world.

Spy agencies work with telecommunications companies to organize and fund special directives to facilitate these missions, which are supervised by a special Foriegn Intelligence Surveillance Court.

> This is where Section 702 starts to become worrisome. While 702 is targeted and is used to collect only a tiny percentage of the world’s communications, it still does involve massive scale. About 100,000 persons are targeted per year. This yields a lot of information. According to a FISA opinion cited by the PCLOB, in 2011, when the number of selectors was smaller than it is today, the government acquired 250 million Internet communications, in addition to telephone conversations. And much of this information relates to persons other than the targets. The Washington Post conservatively estimated that for each targeted person the communications of at least 9 other people are collected.
> By definition, when the government is collecting the communications of a target, it is collecting the communications of those on the other end of the communications.  Even when the government is targeting only non-citizens abroad, it is certain to collect communications to and from citizens and persons in this country -- persons not suspected of any wrongdoing and not suspected of communicating anything of intelligence value.
> The government claims the authority to search those communications whenever a US citizen comes to its attention in circumstances justifying further examination. After all, the government argues, the very first step in any government investigation or analysis is to ask “Do we already have anything in our files about this person?” But searching the 702 repository means that the government is able to retrieve communications of a US person without a court order – communications that the government could not have targeted without a court order in the first place.

https://www.acslaw.org/expertforum/section-702-renewal-opportunities-lost-and-gained/
JIM DEMPSEY Executive Director, Berkeley Center for Law and Technology Jan 29,2018

Furthermore, if a domestic target is identified coincidentally while conducting foriegn surveillance, this justifies a warrant to target that individual for further investigation.

Due to the complexity and senstive nature of surveillance, many questions remain unresolved, with ammendments proposed to the FISA legislation as recenlty as June 2019.

It is significant that such operations are only possible through cooperation with corporations who have - or have the means to acquire - the data in question. Unlike the government, private companies are not held to fourth ammendment standards as they manifest the *extraction imperitave* in their day-to-day ongoings. There are surprisingly few restrictions on what kinds of data a corporation can amass if a user agrees to a privacy policy. On top of all this, after the Snowden compromise, legislation was created to prevent the NSA and other agencies from maintaining their own databases for telecommunications records. Instead, identical records are maintained by the telecommunications company themselves, and government analysts must file special requests to have them run queries.

Cooperation between government agencies and private corporations is more than that - it's actually a fundamental shift in defensive power from public to private sector. This is because the defensive capabilities of a product must be built into the products by the manufacturer. For example, anti-counterfiet technology in high-end laser printers isn't something that the FBI could externally impose upon potential criminals - it has to be part of the printer iteself. This line of logic extends to phones, computers, airplanes, cars - just about anything that could be subject to a hack.

This raises a number of significant questions:

1. What justifies the disparity in authority between government agencies - who seek to protect us - and private corporations - who seek to exploit us?
2. Given the frequency and scale of corporate database compromises, is it wise to trust them with securing this sensitive data, rather than allowing the government to handle it on their own?
3. With more communications happening within corporate platforms - such as Facebook groups and What'sApp - should we trust these companies to set their own policies?
4. Can the intelligence community keep up with the growing variety of communications platforms through ad-hoc directives?

FIX

In the dual context of surveillance capitalism's imperatives as described by Zuboff, alongside Bostrm's vulnerable wold hypothesis, the status quo, semi-anarchic condition of corporate enforcement is clearly insufficient.

Having painted this bleak picture, Bostrom gives four possibilities for achieving 'stabilization:'
> 1. Restrict technological development.
> 2. Ensure that there does not exist a large population of actors representing a wide and recognizably human distribution of motives.
> 3. Establish extremely effective preventitive policing.
> 4. Establish effective global governance.

One variant of the first idea is to allow technological devleopment generally, but restrain developments which are potentially threatening. This, however, violates the random charactaristic of the *urn of discoveries* and drawing the balls. And, of course, it is extraordinarily difficult, and would likely have to be implemented alongside 3 or 4 above. Bostrom does say that this solution is worth attempting.

The second idea involves what Bostrom calls *preference modification* which sounds identical to Zuboff's term *economies of action*. The problem is that in a world of *easy nukes*, even one rogue actor (or relatively small group of them) can ruin things for everybody. The other problem, obviously, is that manipulating everybody in such a way to completely remove any desires for mass destruction would be both difficult and costly (from a moral perspective). Bostom believes that maybe a 5-10% reduction in existential risk might be possible through (what Zuboff calls) *instrumentarianism*.

Thankfully, it's reasonable to expect *easy nukes* style technologies to develop slowly, rather than quickly. For example, 3D printed guns are quite pathetic today, but will eventually become a very serious threat.


## Some Specific Countermeasues and their Limitations

my argument is that surveillance is the core technology which empowers the possible stabilization methods above.

Aside from modifying preferences and altering scientific progress, one could:
> - prevent the dangerous information from spreading
> - restrict access to requisite materials, instruments, and infrastructure
> - deter potential evildoers by increasing the chance of their getting caught
> - be more cautious and do more risk assessment work
> - establish some kind of surveillance and enforcement mechanism that would make it possible to interdict attempts to carry out a destructive act

Bostrom warns that the first three are obviously infeasible, and the last two on their own - although perhaps workable - are insufficient on their own. Surveillance and preventitive policing could avoid an 'easy nukes' scenario when carefully implemented.

Having narrowed down the ineffective countermeasures, Bostrom settles on preventitive policing combined with strong global governance as the most promising approach. Similarly, the authors of "The Future of Violence" see a vast expansion of corporate enforcements dictated by the government. Therefore, I see an emerging, hybrid approach to preventing the rise of 'easy nukes' technologies.

## Hybrid Countermeasures

- First, Manufacturers of grey and black orb technologies are regulated to limit the function within safe parameters.
- Second, Technologies with potential for abuse via modification or hacking are surgically recalled, tracked, and disabled.
- Third, Professionals who need grey orb technologies for their work agree to ongoing surveillance to prevent catastrophes.
- Last, If a widely available technology suddenly becomes threatening, widespread surveillance should be in place to respond immediately.


## Voluntary Surveillance 

Returning to our 3D printer example, imagine a large commercial device which is used by hunderds of corporations in thousands of manufacturing facilities. Assume that such a printer could be used to manufacture dangerous weapons. The manufacturer of this printer would register with a federal agency and maintain a list of all units and their locations, as well as a list of individuals who have permission to operate them. These people would have a security clearance or license, but also be subject to a very high level of surveillance to facilitate preventitive policing. In addition, safegaurds within the device would be in place to prevent the creation of dangerous items.

This kind of agreement is justified because such employees would voluntarily subject themselves to surveillance in exchange for the right to work with the specialized equipment. This would be a step in the right direction, but I must agree with Bostrom that such measures alone would be insuffecient.

## Involuntary Surveillance

The future will also present situations where the general populace has access to black ball technologies. Imagine a scientist discovers a way to manufacture a super-virus using widely available lab equipment, found in universities around the country. In this scenario, the state needs a way to quickly identify individuals who have expressed interest in developing the virus. This would require a pre-existing database of highly sensitive behavioral data to be queried at a moment's notice. Such a database might include communications like phone records and emails, web and search history, geolociation data, and social media data. Intelligence officials could query this database to find individuals who downloaded the instruction manual for creating the virus and therefore have some hope for containing an otherwise hopeless situation.

This of course runs contrary to fourth ammendment standards. The database could very easily be abused for general-purpose law enforcement, or when Bostrom calls 'turn-key totalitarianism' where political dissendents are identified by a ruling power. As I pointed out earlier, much of this data is already collected and analyzed by private corporations for profiteering. Now that we can see the role of surveillance and big data technologies in preventing future catastrophes, I believe it is time for a careful re-evaluation of the cultural and legal constructions which present barriers to these future defense efforts.

## Legal Standards or the State of Exception

Given the high probabilty of a black orb scenario, we may be facing these tough choices sooner than later. However, if we choose not to confront them head-on, we will end up with suspension of normal constitutional law. Jus as the PATRIOT act was passed after the attacks on September 11th, 2001, technologies of empowerment could force the government to declare a state of emergency. As Giorgio Agamben explores in his 2005 book *The State of Exception*, the abrupt suspension of the law in the face of a crisis often comes at the price of human rights violations - such as "enemy combatants" held in Guantanamo Bay. Furthermore, this condition has a tendency to become a prolonged state of being. There is often not a return to normalcy. Surveillance and intelligence operations pose a unique problem, because they are often implemented in secret.

The time to confront technologies of mass empowerment is now, before a catastrophe unfolds and leads us into a state of exception. Even if there is a greatly diminished level of privacy in the general sense, we have a better chance at establishing important - and sustainable - boundaries, and in the process gain a strategic advantage against these emerging threats.
