# The Vulnerable World Hypothesis
Nick Bostrom, "The Vulnerable World Hypothesis," Working Paper, v. 3.44 (2018), https://nickbostrom.com/papers/vulnerable.pdf

## Introduction

In this paper, Oxford philosopher Nick Bostrom makes three key arguments:

- In the near future, more people will have easier access to technologies which can cause catastrophic destruction.
- This vulnerable world can only be stabilized through a vast expansion of policing and global governance.
- We must evaluate the costs and risks of ubiquitous surveillance and/or a unified global order in context of the vulnerable world hypothesis.

To warrant the first argument, Bostrom posits "The Urn of Inventions." One can imagine an urn with varying colors of balls - white, grey, and black. Each time we create a new invention or make a new discovery, humanity reaches into the urn and draws out a ball. Whiter balls have a positive effect on society with few negative side effects. As they become darker grey, however, the positive effects are lessened, or the drawbacks are increased. Finally, we have the black balls, which represent inventions which - absent outside intervention -  cause catastrophic damage and the end of civilization.

Thankfully, the overall net impact of inventions has been overwhelmingly positive for humanity. We have drawn some dark grey balls, however, such as the nuclear bomb, anthrax, and the like. These technologies, however, have been restrained in their application, primarily because of the difficulty of developing them. No rogue individual or small sect can create a nuclear weapon. 

In the urn of inventions, however, it is inevitable that we will one day draw one or more black balls which are not only extremely dangerous, but also easy to build. Bostom calls these "easy nukes." If Szilard and Einstien had supsected that nuclear weapons would have been low effort to build, their options would have been very limited. They could have evangelized for a ban on studying nuclear physics. And, even if this was politically feasible, in practical reality, another nation would have made the discovery, or the secret would have gotten out.

## The Vulnerable World Hypothesis

After setting the stage, Bostrom shares his formal definition.

> Intuitively, the hypothesis is that there is some level of technology at which civilization almost certainly gets destroyed unless quite extraordinary and historically unprecedented degrees of preventive policing and/or global  governance are implemented. More precisely:
>>
> -VWH: If technological development continues then a set of capabilities will at some point be attained that make the devastation of civilization extremely likely, unless civilization sufficiently *exits* the semi-anarchic default condition. (pg 6)

The "semi-anarchic default conditions" are:

> - Limited capacity for preventive policing. States do not have sufficiently reliable means of real-time surveillance and interception to make it virtually impossible for any individual or small group within their territory to carry out illegal actions—particularly actions that are very strongly disfavored by >99% of the population.
> - Limited capacity for global governance. There is no reliable mechanism for solving global coordination problems and protecting global commons—particularly in high-stakes situations where vital national security interests are involved.
> - Diverse motivations. There is a wide and recognizably human distribution of motives represented by a large population of actors (at both the individual and state level)—in particular, there are many actors motivated, to a substantial degree, by perceived self-interest (e.g. money, power, status, comfort and convenience) and there are some actors (“the apocalyptic residual”) who would act in ways that destroy civilization even at high cost to themselves. (pg 7)

This leads us to the key concern: Do the risks of continuing in the semi-anarchic state outweigh the cost of the radical developments needed in order to impose a high level of surveillance and policing?

In order to approach this, we have to postulate how badly a black ball would harm society. Bostrom draws a hard line in the sand in regards to a potential "catastrophe" - a single event which immediately wipes out 15% or more of the earth's population, or >50% of global GDP. This definition, of course, is not something he would advocate, but is used for illustritave purposes when discussing the nuances of the hypothesis as the paper progresses.

Another key point is that Bostrom states he is not trying to determine if the VWH is true or not, but rather to explain it thoroughly. He does stipulate, however, that it would be difficult to convince him that it is *untrue.*

## Topology of Vulnerabilities

Bostrom outlines five kinds of vulnerabilities:

1. *Easy nukes* in which one or more technologies emerges in the semi-anarchic default condition, allowing any rogue actor to cause widespread destruction.
2. *Safe first strike* in which a state actor develops a technology which ensures dominance in a first-strike scenario, without threat of retaliation. For example, imagine nuclear weapons with no mutually-assured-destruction.
3. (Referred to simply as 2-a) in which powerful actors can produce devastating weapons, and have enough incentive to do so, even in the face of retaliation and other consequences.
4. *Worse global warming* in which individual actors are incentivized to contribute to a destruction scenario (yet do not face adequate and immediate negative consequences), and the collective damages lead to catastrophe. This could happen whether the threat is understood by the individuals or not.
5. *Surprising strangelets* in which a technology carries a hidden risk such that default outcome when it is discovered is inadvertent destruction. For example, if the first nuclear bomb test had ignited the atmosphere on fire.

## Achieving Stabilization

Having painted this bleak picture, Bostrom gives four possibilities for achieving 'stabilization:'
> 1. Restrict technological development.
> 2. Ensure that there does not exist a large population of actors representing a wide and recognizably human distribution of motives.
> 3. Establish extremely effective preventitive policing.
> 4. Establish effective global governance.

One variant of the first idea is to allow technological devleopment generally, but restrain developments which are potentially threatening. This, however, violates the random charactaristic of the *urn of discoveries* and drawing the balls. And, of course, it is extraordinarily difficult, and would likely have to be implemented alongside 3 or 4 above. Bostrom does say that this solution is worth attempting.

The second idea involves what Bostrom calls *preference modification* which sounds identical to Zuboff's term *economies of action*. The problem is that in a world of *easy nukes*, even one rogue actor (or relatively small group of them) can ruin things for everybody. The other problem, obviously, is that manipulating everybody in such a way to completely remove any desires for mass destruction would be both difficult and costly (from a moral perspective). Bostom believes that maybe a 5-10% reduction in existential risk might be possible through (what Zuboff calls) *instrumentarianism*.

Thankfully, it's reasonable to expect *easy nukes* style technologies to develop slowly, rather than quickly. Therefore:

> These considerations notwithstanding, preference modification could be helpful in scenarios in which the set of empowered actors is initially limited to some small definable subpopulation. Some black-ball technologies, when they first emerge from the urn, might be difficult to use and require specialized equipment. There could be a period of several years before such a technology has been perfected to the point where an average individual could master it. During
this early period, the set of empowered actors could be quite limited; for example, it might consist exclusively of individuals with bioscience expertise working in a particular type of lab. Closer screening of applicants to positions in such labs could then make a meaningful dent in the risk that a destructive individual gains access to the biotech black ball within the first few years of its emergence. And that reprieve may offer an opportunity to introduce other countermeasures to provide more lasting stabilization, in anticipation of the time when the technology gets easy enough to use that it diffuses to a wider population. 

# Some Specific Countermeasues and their Limitations

Because my focus is on surveillance, I have omitted Bostrom's discussion on destruction scenarios 2, 3, 4, and 5 (at least for the moment). We will see later, however, that surveillance is a core technology which empowers the possible stabilization methods above.

Aside from modifying preferences and altering scientific progress, one could:
> - prevent the dangerous information from spreading
> - restrict access to requisite materials, instruments, and infrastructure
> - deter potential evildoers by increasing the chance of their getting caught
> - be more cautious and do more risk assessment work
> - establish some kind of surveillance and enforcement mechanism that would make it possible to interdict attempts to carry out a destructive act

Bostrom warns that the first three are obviously infeasible, and the last two on their own - although perhaps workable - are insufficient on their own.







